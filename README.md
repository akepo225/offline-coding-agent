# Offline Coding Agent

> Autonomous AI coding assistant that operates completely offline on restricted Windows business laptops

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Windows](https://img.shields.io/badge/platform-Windows-lightgrey.svg)](https://www.microsoft.com/windows)
[![Model: Qwen2.5-Coder-7B](https://img.shields.io/badge/model-Qwen2.5--Coder--7B-green.svg)](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)
[![No Admin Rights Required](https://img.shields.io/badge/no%20admin%20rights-required-brightgreen.svg)](https://github.com/akepo225/offline-coding-agent)
[![Production Ready](https://img.shields.io/badge/status-production%20ready-success.svg)](https://github.com/akepo225/offline-coding-agent)

## ğŸ¯ Overview

This project provides a **production-ready autonomous AI coding assistant** specifically designed for **restricted Windows business laptops** with **no admin rights**. It uses **Qwen2.5-Coder-7B** with **llama-cpp-python** to deliver professional-grade AI assistance with **multi-step task execution** without requiring internet connectivity, VS Code extensions, or system installations.

**Key Differentiator:** Unlike simple chat interfaces, this agent **autonomously executes multiple tools** in sequence to complete complex tasks without user intervention.

## âœ¨ Key Features

- **ğŸ¤– Autonomous Execution**: Multi-step task completion with tool chaining (5-7 tools per session)
- **ğŸ”’ Complete Offline Operation**: No internet required after initial setup
- **ğŸš« No Admin Rights Required**: Runs entirely with user permissions
- **ğŸ’» Windows & Linux Compatible**: Designed for business laptops (32GB RAM recommended)
- **ğŸ§  Efficient AI Model**: Qwen2.5-Coder-7B (4.4GB) with 32K context window
- **ğŸ”§ 5 Built-in Tools**: read_file, write_file, create_directory, execute_python, run_command
- **ğŸ¯ 95% Success Rate**: Proven autonomous capabilities with comprehensive testing
- **ğŸ“Š CPU-Optimized**: Runs on CPU via llama-cpp-python (no GPU required)
- **ğŸ¨ Smart Feedback Loop**: Full content feedback enables context-aware multi-step tasks
- **ğŸ“ Portable**: Can run from USB drive or local directory

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Autonomous Agent Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         working_assistant.py                         â”‚  â”‚
â”‚  â”‚  - 2-Iteration Autonomous Loop                       â”‚  â”‚
â”‚  â”‚  - XML-Structured Prompts                            â”‚  â”‚
â”‚  â”‚  - Full Content Feedback                             â”‚  â”‚
â”‚  â”‚  - Smart Termination Logic                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Tool Execution Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ read_file   â”‚ write_file  â”‚ execute_     â”‚ run_     â”‚  â”‚
â”‚  â”‚             â”‚             â”‚ python       â”‚ command  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚ create_directory                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Local AI Engine                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Qwen2.5-Coder-7B    â”‚  â”‚    llama-cpp-python          â”‚  â”‚
â”‚  â”‚ (4.4GB Q4_K_M)      â”‚  â”‚    (CPU Inference)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- **OS**: Windows 10/11 or Linux
- **RAM**: 8GB minimum, 32GB recommended
- **Python**: 3.8 or higher
- **Storage**: 6GB free space (5GB for model + 1GB for dependencies)
- **Permissions**: Standard user account (no admin rights required)

### Installation (No Admin Rights Required)

1. **Clone Repository**
   ```bash
   git clone https://github.com/akepo225/offline-coding-agent.git
   cd offline-coding-agent
   ```

2. **Download AI Model**
   ```bash
   # Automated model download (4.4GB)
   python scripts/download_model.py

   # Downloads: Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf
   # From: bartowski/Qwen2.5-Coder-7B-Instruct-GGUF on Hugging Face
   ```

3. **Install Dependencies Locally**
   ```bash
   # Create virtual environment (optional but recommended)
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate

   # Install minimal requirements
   pip install --user -r requirements.txt
   # Installs: llama-cpp-python, rich, PyYAML, requests, tqdm, huggingface-hub
   ```

4. **Verify Installation**
   ```bash
   python scripts/verify_installation.py
   ```

## ğŸ“– Usage

### Quick Launch
```bash
# Start the assistant with default model
./start_assistant.sh

# Or run directly
python working_assistant.py --model models/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf
```

### Interactive Mode
```bash
# Start interactive session
./start_assistant.sh

ğŸ¤– AI Assistant Ready
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Try complex multi-step tasks:
> Create a directory called 'project', write a Python script 'project/calculator.py'
  with add and multiply functions, execute it to test, and write a README

# The assistant will autonomously:
# 1. Create the directory
# 2. Write the code file
# 3. Execute the code to verify
# 4. Write the README with results
# All without additional prompts!

# Special Commands:
/add file.py         # Add file to context
/remove file.py      # Remove from context
/list                # List context files
/clear               # Clear conversation
/tools               # Show available tools
/auto                # Toggle auto-confirm mode
/quit                # Exit
```

### Single Prompt Mode
```bash
# Execute a task and exit
python working_assistant.py \
  --model models/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf \
  --prompt "Create a Python script that calculates Fibonacci numbers and test it" \
  --auto-confirm

# With file context
python working_assistant.py \
  --model models/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf \
  --files config.py utils.py \
  --prompt "Refactor these files to use modern Python patterns"
```

### Example Tasks
```bash
# Example 1: Project scaffolding
> Create src/ and tests/ directories with __init__.py files and a README

# Example 2: Code generation with testing
> Write a Python script that validates email addresses,
  save it as validator.py, then create test_validator.py with unit tests

# Example 3: File processing
> Read all .py files in src/, analyze them, and write a summary to ARCHITECTURE.md

# Example 4: Code execution
> Create a data analysis script that reads data.csv,
  calculates statistics, and saves results to report.txt
```

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| **Model Size** | 4.4GB (Q4_K_M quantized) |
| **RAM Usage** | ~8GB during inference |
| **Response Time** | 5-10 seconds per iteration |
| **Inference Speed** | 15-30 tokens/second (CPU) |
| **Context Window** | 4096 tokens (configurable up to 32K) |
| **Success Rate** | 95% (21/22 tool executions) |
| **Tool Chaining** | 5-7 tools per autonomous session |
| **Iterations** | 2-iteration autonomous loop |

## ğŸ—‚ï¸ Project Structure

```
offline-coding-agent/
â”œâ”€â”€ working_assistant.py          # Main autonomous AI assistant
â”œâ”€â”€ start_assistant.sh            # Quick launch script
â”œâ”€â”€ CLAUDE.md                     # Instructions for Claude Code
â”œâ”€â”€ requirements.txt              # Minimal Python dependencies
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml             # Model and behavior configuration
â”œâ”€â”€ docs/                         # Comprehensive documentation
â”‚   â”œâ”€â”€ COMPACT_SUMMARY.md       # Quick reference guide
â”‚   â”œâ”€â”€ AUTONOMOUS_LOOP_FINAL_REPORT.md  # Production readiness report
â”‚   â”œâ”€â”€ AUTONOMOUS_EXECUTION_*.md  # Implementation details
â”‚   â”œâ”€â”€ concept.md               # Project concept
â”‚   â”œâ”€â”€ model_evaluation.md      # Model selection research
â”‚   â”œâ”€â”€ implementation_guide.md  # Setup guide
â”‚   â”œâ”€â”€ technical_architecture.md # Architecture details
â”‚   â””â”€â”€ github/                  # GitHub-related documentation
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_model.py        # Model download automation
â”‚   â””â”€â”€ verify_installation.py   # Installation verification
â”œâ”€â”€ test_logs/                    # Test session documentation
â”‚   â”œâ”€â”€ autonomous_test_session_*.md  # Test results
â”‚   â””â”€â”€ autonomous_test_session_final_assessment.md
â”œâ”€â”€ src/                          # Legacy CLI (historical)
â”‚   â”œâ”€â”€ cli.py                   # Click-based CLI interface
â”‚   â””â”€â”€ utils/                   # Config and formatting utilities
â”œâ”€â”€ models/                       # AI model files (gitignored)
â”‚   â””â”€â”€ Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf (4.4GB)
â”œâ”€â”€ archive/                      # Previous assistant versions
â”œâ”€â”€ tests/                        # Test files
â””â”€â”€ examples/                     # Usage examples
```

## ğŸ› ï¸ Advanced Configuration

### Hardware-Specific Configurations

The assistant comes with pre-configured profiles for different hardware levels:

**Default (Balanced) - `config/default.yaml`**
```bash
./start_assistant.sh --config config/default.yaml
```
- Context: 4096 tokens
- Max Tokens: 1024
- Batch Size: 512
- Best for: 16GB RAM systems

**High Performance - `config/high_performance.yaml`**
```bash
./start_assistant.sh --config config/high_performance.yaml
```
- Context: 8192 tokens (2x default)
- Max Tokens: 2048 (2x default)
- Batch Size: 1024 (2x default)
- Best for: 32GB+ RAM, modern CPUs
- Ideal for: Complex multi-step tasks, large codebases

**Low-End - `config/low_end.yaml`**
```bash
./start_assistant.sh --config config/low_end.yaml
```
- Context: 2048 tokens (Â½ default)
- Max Tokens: 512 (Â½ default)
- Batch Size: 256 (Â½ default)
- Best for: 8GB RAM systems, older CPUs
- Ideal for: Quick tasks, fast responses

### Custom Configuration

Edit any config file to customize inference parameters:

```yaml
inference:
  # Context window size (tokens) - affects memory usage
  # Low: 2048 | Mid: 4096 | High: 8192-16384
  n_ctx: 4096

  # CPU threads (0 = auto-detect)
  n_threads: 0

  # Batch size - affects processing speed
  # Low: 256 | Mid: 512 | High: 1024
  n_batch: 512

  # Maximum tokens per response
  # Fast: 512 | Balanced: 1024 | Quality: 2048-4096
  max_tokens: 1024

  # Temperature (0.0-1.0) - controls randomness
  # Code: 0.2-0.3 | Balanced: 0.5-0.7 | Creative: 0.8-1.0
  temperature: 0.3

  # Top-p sampling (0.0-1.0)
  top_p: 0.9

  # Repeat penalty (1.0-1.5) - prevents repetition
  repeat_penalty: 1.1
```

**Performance Impact:**
- â¬†ï¸ Higher `n_ctx` = More context, more RAM usage
- â¬†ï¸ Higher `max_tokens` = Longer responses, slower generation
- â¬†ï¸ Higher `n_batch` = Faster processing, more RAM usage
- â¬†ï¸ Higher `temperature` = More creative, less deterministic

### Tool Customization
Add new tools in `working_assistant.py`:
```python
def tool_<name>(self, arg1, arg2=default):
    """Tool description."""
    try:
        # Your tool implementation
        return {"success": True, "data": result}
    except Exception as e:
        return {"success": False, "error": str(e)}
```

Tools are auto-discovered and made available to the AI.

### Autonomous Loop Tuning
Modify `working_assistant.py` line 433:
```python
max_iterations = 2  # Increase for more complex tasks
```

**Trade-offs:**
- Higher iterations = more complex tasks completed
- Higher iterations = longer execution time
- Recommended: 2-3 iterations for most tasks

## ğŸ“‹ Status & Roadmap

### âœ… Completed (Production Ready)
- [x] Repository setup and comprehensive documentation
- [x] Model selection and integration (Qwen2.5-Coder-7B)
- [x] Autonomous AI assistant with tool execution
- [x] Multi-step task completion (2-iteration loop)
- [x] 5 core tools: read, write, create, execute, command
- [x] Automated model download scripts
- [x] Performance benchmarking (95% success rate)
- [x] llama-cpp-python CPU optimization
- [x] Interactive terminal interface with Rich formatting
- [x] XML-structured prompts for agent guidance
- [x] Full content feedback loop
- [x] Smart termination logic
- [x] Comprehensive testing and logging

### ğŸš§ Optional Enhancements
- [ ] Retry logic with exponential backoff for API failures
- [ ] Conversation memory management (summarization)
- [ ] Additional tools (web browsing, API calls, database ops)
- [ ] Event streaming for real-time UI updates
- [ ] Specialized agents for domain-specific tasks
- [ ] Self-contained portable package
- [ ] USB drive deployment option

### ğŸ“Š Current Status
**Grade:** A+ | **Success Rate:** 95% | **Status:** Production Ready

See [docs/COMPACT_SUMMARY.md](docs/COMPACT_SUMMARY.md) for detailed status and [docs/AUTONOMOUS_LOOP_FINAL_REPORT.md](docs/AUTONOMOUS_LOOP_FINAL_REPORT.md) for test results.

## ğŸ¤ Community

- **Discussions**: [GitHub Discussions](https://github.com/your-org/offline-coding-agent/discussions)
- **Issues**: [GitHub Issues](https://github.com/your-org/offline-coding-agent/issues)
- **Wiki**: [Project Wiki](https://github.com/your-org/offline-coding-agent/wiki)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Qwen Team** for the excellent Qwen2.5-Coder-7B model
- **llama-cpp-python** for efficient CPU inference bindings
- **Google ADK** for loop agent patterns and termination strategies
- **Reddit "Baby Manus" Tutorial** for full content feedback inspiration
- **Hugging Face** for model hosting and distribution
- **Rich** for beautiful terminal formatting

## ğŸ”— Related Projects & Resources

- [Qwen2.5-Coder-7B](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct) - The AI model
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) - Python bindings for llama.cpp
- [Google ADK Loop Agents](https://google.github.io/adk-docs/agents/workflow-agents/loop-agents/) - Agent design patterns
- [Rich](https://github.com/Textualize/rich) - Terminal formatting library

## ğŸ”’ Security & Compliance

- **No network access required** after initial setup
- **No admin privileges needed** for installation or usage
- **All processing happens locally** on the user's machine
- **Suitable for air-gapped and high-security environments**

---

**Built for professional developers working in highly restricted Windows environments.**

Need help? Check our [documentation](docs/) or open an [issue](https://github.com/your-org/offline-coding-agent/issues).